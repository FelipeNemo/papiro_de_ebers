"""
MODULO PIPELINE
Processa PDFs/TXT de prontu√°rios eletr√¥nicos, busca conceitos SNOMED CT
usando embeddings locais e retorna diagn√≥sticos simulados via LLM.

Autor: Felipe Augusto Bastista Mendes dos Santos
Stakeholder: Ebers Sa√∫de
Data: 2024-08-30
"""

import os
import re
import glob
import json
import time
import pdfplumber
import pandas as pd
import numpy as np
from typing import TypedDict, List, Dict
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langsmith import traceable
from langchain_text_splitters import RecursiveCharacterTextSplitter

from dotenv import load_dotenv
import os

load_dotenv(dotenv_path=".env")
print(os.getenv("OPENAI_API_KEY"))  # deve imprimir a chave

# -------------------------------
# Inicializa√ß√£o de modelos
# -------------------------------
print("üîπ Inicializando modelo de embeddings...")
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# -------------------------------
# Utilit√°rios
# -------------------------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def extract_and_split_file(file_path: str, snomed_terms: List[str], chunk_size: int = 150, chunk_overlap: int = 30) -> List[str]:
    """L√™ PDF ou TXT, gera chunks e filtra por termos SNOMED, mas retorna todos se nenhum match."""
    ext = os.path.splitext(file_path)[1].lower()
    text = ""

    if ext == ".pdf":
        with pdfplumber.open(file_path) as pdf:
            text = " ".join(page.extract_text() or "" for page in pdf.pages)
    elif ext == ".txt":
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
    else:
        raise ValueError(f"Formato de arquivo n√£o suportado: {ext}")

    text = clean_text(text)
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)

    # Filtra chunks que cont√™m termos SNOMED
    term_chunks = [chunk for chunk in chunks if any(term.lower() in chunk.lower() for term in snomed_terms)]
    # TODO: Evitar chunks irrelevantes
    # Se n√£o houver matches, n√£o retornar todos, mas apenas os primeiros 1-2 chunks para debug
    return chunks
    # Se n√£o houver matches, retorna todos os chunks (para debug ou fallback)
    #return term_chunks or chunks

def generate_questions(concepts: List[Dict]) -> List[str]:
    return [f"O paciente apresenta {c.get('term')}?" for c in concepts if c.get("term")]

def build_prompt(concepts: List[Dict], chunk: str) -> str:
    context = (
        "Voc√™ √© um assistente cl√≠nico especializado em terminologias m√©dicas (SNOMED CT). "
        "Sua tarefa √© analisar trechos de prontu√°rios m√©dicos e sugerir diagn√≥sticos potenciais."
    )
    instructions = (
        "Siga estas etapas:\n"
        "1. Leia o trecho do prontu√°rio.\n"
        "2. Analise os conceitos SNOMED CT recuperados.\n"
        "3. Escolha o conceito mais relevante.\n"
        "4. Justifique brevemente sua escolha.\n"
        "5. Retorne no formato estruturado JSON."
    )
    input_section = f"Trecho: {chunk}\nConceitos candidatos: {concepts}"
    output_section = "Formato esperado:\n{ 'diagnostico': 'nome do conceito SNOMED CT', 'justificativa': 'texto curto' }"
    return f"{context}\n\n{instructions}\n\n{input_section}\n\n{output_section}"

def build_qna(chunk: str, questions: List[str]) -> List[Dict]:
    qna_results = []
    for q in questions:
        prompt = f"""
Voc√™ √© um assistente cl√≠nico. Leia atentamente o trecho do prontu√°rio abaixo.
Responda apenas com 'Sim', 'N√£o' ou 'N√£o mencionado'.

Trecho: {chunk}

Pergunta: {q}

Instru√ß√µes:
- Responda 'Sim' somente se houver **evid√™ncia textual clara** no trecho que confirma o conceito.
- Responda 'N√£o' se houver evid√™ncia textual que negue o conceito.
- Responda 'N√£o mencionado' se n√£o houver informa√ß√µes suficientes no trecho.

Responda **apenas com uma destas tr√™s op√ß√µes**, sem explica√ß√µes adicionais.
"""
        response = llm.invoke(prompt).content.strip()
        qna_results.append({"pergunta": q, "resposta": response})
    return qna_results

def evaluate_results(results: List[Dict]) -> Dict:
    start = time.time()
    total = sum(len(r.get("qna", [])) for r in results)
    null_count = sum(1 for r in results for q in r.get("qna", []) if "N√£o mencionado" in q["resposta"])
    return {"total": total, "nulos": null_count, "ratio_nulos": null_count/total if total>0 else 0, "tempo_execucao": time.time() - start}

# -------------------------------
# Setup Vector Store FAISS
# -------------------------------
import faiss

index = faiss.read_index("data/vector/snomed_ivf.index", faiss.IO_FLAG_MMAP)
index.nprobe = 16
concept_groups = pd.read_csv("data/vector/snomed_concepts.csv")

# TODO: 
# Sem filtro por categoria
snomed_terms = concept_groups["term"].dropna().tolist()


class PipelineState(TypedDict):
    pdf_path: str
    chunks: List[str]
    results: List[Dict]
    questions_map: List[Dict]
    qna_results: List[Dict]
    metrics: Dict

 # ajustar distance_threshold conforme os resultados (0.5‚Äì0.7 costuma funcionar bem).
# ajustar distance_threshold conforme os resultados (0.5‚Äì0.7 costuma funcionar bem).
def search_snomed(query: str, k: int = 5, distance_threshold: float = 0.6) -> List[Dict]: 
    emb = np.array(embedding_model.encode([query])).astype("float32")
    D, I = index.search(emb, k)

    results = []  # inicializa a lista de resultados

    # Filtra conceitos apenas pelo threshold de dist√¢ncia
    for idx, dist in zip(I[0], D[0]):
        # FAISS retorna similaridade invertida dependendo do √≠ndice: use <= para IVF ou 1/dist
        if dist <= distance_threshold:
            results.append(concept_groups.iloc[idx].to_dict())

    # Se quiser sempre pelo menos 1 resultado, mesmo que distante:
    if not results and len(I[0]) > 0:
        results.append(concept_groups.iloc[I[0][0]].to_dict())

    return results



# -------------------------------
# LangGraph Nodes
# -------------------------------
@traceable(name="Input Node")
def input_node(state: PipelineState):
    return {"pdf_path": state["pdf_path"]}

@traceable(name="Process Node")
def process_node(state: PipelineState):
    return {"chunks": extract_and_split_file(state["pdf_path"], snomed_terms, 100, 20)}


def retriever_node(state: PipelineState):
    return {"results": [{"chunk": chunk, "matches": search_snomed(chunk, k=5, distance_threshold=0.8)} for chunk in state["chunks"]]}

@traceable(name="LLM Node")
def llm_node(state: PipelineState):
    analyzed = []
    for r in state["results"]:
        prompt = build_prompt(r["matches"], r["chunk"])
        analyzed.append({"chunk": r["chunk"], "concepts": r["matches"], "diagnostic": llm.invoke(prompt).content})
    return {"results": analyzed}

@traceable(name="Question Node")
def question_node(state: PipelineState):
    return {"results": state["results"], "questions_map": [{"chunk": r["chunk"], "questions": generate_questions(r["concepts"])} for r in state["results"]]}

@traceable(name="QnA Node")
def qna_node(state: PipelineState):
    return {"qna_results": [{"chunk": qm["chunk"], "qna": build_qna(qm["chunk"], qm["questions"])} for qm in state["questions_map"]]}

@traceable(name="Evaluation Node")
def evaluation_node(state: PipelineState):
    metrics = evaluate_results(state["qna_results"])
    return {"qna_results": state["qna_results"], "metrics": metrics}

# -------------------------------
# Grafo LangGraph
# -------------------------------
graph = StateGraph(PipelineState)
nodes = [("input", input_node), ("process", process_node), ("retriever", retriever_node), 
         ("llm", llm_node), ("questions", question_node), ("qna", qna_node), ("evaluation", evaluation_node)]

for name, node in nodes:
    graph.add_node(name, node)

graph.set_entry_point("input")
edges = [("input","process"),("process","retriever"),("retriever","llm"),("llm","questions"),
         ("questions","qna"),("qna","evaluation"),("evaluation",END)]

for src, dst in edges:
    graph.add_edge(src,dst)

app = graph.compile()

# -------------------------------
# Execu√ß√£o da Pipeline
# -------------------------------
@traceable(name="SNOMED Pipeline")
def run_pipeline(pdf_path: str):
    return app.invoke({"pdf_path": pdf_path})

# -------------------------------
# Batch Execution
# -------------------------------
input_folder = "data/test"
res_folder = os.path.join(input_folder, "res")
os.makedirs(res_folder, exist_ok=True)

files = glob.glob(os.path.join(input_folder, "*.txt")) + glob.glob(os.path.join(input_folder, "*.pdf"))

for file_path in files:
    print(f"Processando {file_path}...")
    final_state = run_pipeline(file_path)
    output_path = os.path.join(res_folder, os.path.splitext(os.path.basename(file_path))[0] + ".json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(final_state, f, ensure_ascii=False, indent=2)
    print(f"Resultado salvo em {output_path}\n")
