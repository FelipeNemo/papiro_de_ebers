"""
OtimizaÃ§Ãµes FAISS para RTX 3060 Ti
Implementa tuning prÃ¡tico para mÃ¡xima performance
"""

import faiss
import numpy as np
import torch
import os
from typing import Tuple, Optional
import time

class OptimizedFAISSGPU:
    """OtimizaÃ§Ãµes FAISS para RTX 3060 Ti"""
    
    def __init__(self, device: str = "cuda"):
        """
        Inicializa otimizaÃ§Ãµes FAISS
        
        Args:
            device: Dispositivo para processamento
        """
        self.device = device
        self.gpu_resources = None
        self.index = None
        
        # ConfiguraÃ§Ãµes otimizadas para RTX 3060 Ti
        self.config = {
            "nlist": 224,  # âˆš50k â‰ˆ 224
            "nprobe": 16,  # 16-32 para boa precisÃ£o
            "m": 48,       # Para PQ
            "nbits": 8,    # Para PQ
            "use_gpu": True,
            "gpu_memory_fraction": 0.8,  # 80% da GPU
            "warmup_queries": 10
        }
        
        print(f"ğŸš€ FAISS GPU Otimizado para RTX 3060 Ti")
        print(f"ğŸ¯ Dispositivo: {self.device}")
        
    def setup_gpu_resources(self):
        """Configura recursos GPU"""
        if self.device == "cuda" and torch.cuda.is_available():
            print("ğŸ”§ Configurando recursos GPU...")
            
            # Verifica se FAISS GPU estÃ¡ disponÃ­vel
            try:
                import faiss
                if hasattr(faiss, 'StandardGpuResources'):
                    # Configura GPU
                    self.gpu_resources = faiss.StandardGpuResources()
                    
                    # Configura memÃ³ria GPU
                    self.gpu_resources.setTempMemory(
                        int(self.config["gpu_memory_fraction"] * torch.cuda.get_device_properties(0).total_memory)
                    )
                    
                    print(f"âœ… GPU configurada: {torch.cuda.get_device_name(0)}")
                    print(f"ğŸ’¾ MemÃ³ria alocada: {self.config['gpu_memory_fraction']*100:.0f}%")
                else:
                    print("âš ï¸ FAISS GPU nÃ£o disponÃ­vel, usando CPU")
                    self.device = "cpu"
                    self.gpu_resources = None
            except Exception as e:
                print(f"âš ï¸ Erro ao configurar GPU: {e}, usando CPU")
                self.device = "cpu"
                self.gpu_resources = None
            
            # Configura threads
            torch.set_num_threads(1)  # Evita conflitos
        else:
            print("âš ï¸ GPU nÃ£o disponÃ­vel, usando CPU")
            self.device = "cpu"
            self.gpu_resources = None
            
    def create_optimized_index(self, dimension: int, num_vectors: int) -> faiss.Index:
        """
        Cria Ã­ndice FAISS otimizado
        
        Args:
            dimension: DimensÃ£o dos vetores
            num_vectors: NÃºmero de vetores
            
        Returns:
            Ãndice FAISS otimizado
        """
        print(f"ğŸ”¨ Criando Ã­ndice FAISS otimizado...")
        print(f"   ğŸ“Š DimensÃ£o: {dimension}")
        print(f"   ğŸ“Š Vetores: {num_vectors}")
        
        # Escolhe tipo de Ã­ndice baseado no tamanho
        if num_vectors <= 100000:
            # IVF-Flat para â‰¤100k
            print("   ğŸ“Š Usando IVF-Flat (â‰¤100k vetores)")
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFFlat(quantizer, dimension, self.config["nlist"])
        else:
            # IVF-PQ para >100k
            print("   ğŸ“Š Usando IVF-PQ (>100k vetores)")
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFPQ(quantizer, dimension, self.config["nlist"], self.config["m"], self.config["nbits"])
            
        # Move para GPU se disponÃ­vel
        if self.device == "cuda" and self.gpu_resources and hasattr(faiss, 'index_cpu_to_gpu'):
            print("   ğŸ¯ Movendo para GPU...")
            index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, index)
        else:
            print("   ğŸ–¥ï¸ Usando CPU")
            
        self.index = index
        print("âœ… Ãndice criado!")
        
        return index
        
    def train_index(self, vectors: np.ndarray):
        """Treina Ã­ndice com vetores"""
        print("ğŸ“ Treinando Ã­ndice...")
        
        # Normaliza vetores (L2)
        vectors = self._normalize_vectors(vectors)
        
        # Treina Ã­ndice
        start_time = time.time()
        self.index.train(vectors.astype('float32'))
        train_time = time.time() - start_time
        
        print(f"âœ… Ãndice treinado em {train_time:.2f}s")
        
    def add_vectors(self, vectors: np.ndarray):
        """Adiciona vetores ao Ã­ndice"""
        print("ğŸ“ Adicionando vetores ao Ã­ndice...")
        
        # Normaliza vetores
        vectors = self._normalize_vectors(vectors)
        
        # Adiciona vetores
        start_time = time.time()
        self.index.add(vectors.astype('float32'))
        add_time = time.time() - start_time
        
        print(f"âœ… {len(vectors)} vetores adicionados em {add_time:.2f}s")
        
    def search(self, query_vectors: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Busca no Ã­ndice
        
        Args:
            query_vectors: Vetores de consulta
            k: NÃºmero de resultados
            
        Returns:
            (scores, indices)
        """
        # Normaliza vetores de consulta
        query_vectors = self._normalize_vectors(query_vectors)
        
        # Configura nprobe
        if hasattr(self.index, 'nprobe'):
            self.index.nprobe = self.config["nprobe"]
            
        # Busca
        start_time = time.time()
        scores, indices = self.index.search(query_vectors.astype('float32'), k)
        search_time = time.time() - start_time
        
        return scores, indices, search_time
        
    def warmup(self, num_queries: int = 10):
        """Faz warm-up do Ã­ndice"""
        print(f"ğŸ”¥ Fazendo warm-up com {num_queries} consultas...")
        
        if self.index is None:
            print("âš ï¸ Ãndice nÃ£o criado, pulando warm-up")
            return
            
        # Gera consultas aleatÃ³rias
        dimension = self.index.d
        query_vectors = np.random.randn(num_queries, dimension).astype('float32')
        
        # Busca de warm-up
        for i in range(num_queries):
            self.search(query_vectors[i:i+1], k=10)
            
        print("âœ… Warm-up concluÃ­do!")
        
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """Normaliza vetores L2"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1  # Evita divisÃ£o por zero
        return vectors / norms
        
    def save_index(self, path: str):
        """Salva Ã­ndice"""
        print(f"ğŸ’¾ Salvando Ã­ndice em: {path}")
        
        if self.device == "cuda" and self.gpu_resources and hasattr(faiss, 'index_gpu_to_cpu'):
            # Move para CPU antes de salvar
            cpu_index = faiss.index_gpu_to_cpu(self.index)
            faiss.write_index(cpu_index, path)
        else:
            faiss.write_index(self.index, path)
            
        print("âœ… Ãndice salvo!")
        
    def load_index(self, path: str):
        """Carrega Ã­ndice"""
        print(f"ğŸ“‚ Carregando Ã­ndice de: {path}")
        
        # Carrega Ã­ndice
        self.index = faiss.read_index(path)
        
        # Move para GPU se disponÃ­vel
        if self.device == "cuda" and self.gpu_resources and hasattr(faiss, 'index_cpu_to_gpu'):
            print("   ğŸ¯ Movendo para GPU...")
            self.index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, self.index)
        else:
            print("   ğŸ–¥ï¸ Usando CPU")
            
        print("âœ… Ãndice carregado!")
        
    def get_index_stats(self) -> dict:
        """Retorna estatÃ­sticas do Ã­ndice"""
        if self.index is None:
            return {}
            
        stats = {
            "dimension": self.index.d,
            "total_vectors": self.index.ntotal,
            "is_trained": self.index.is_trained,
            "device": self.device
        }
        
        if hasattr(self.index, 'nlist'):
            stats["nlist"] = self.index.nlist
        if hasattr(self.index, 'nprobe'):
            stats["nprobe"] = self.index.nprobe
            
        return stats
        
    def benchmark(self, query_vectors: np.ndarray, k: int = 10, num_runs: int = 100):
        """Faz benchmark do Ã­ndice"""
        print(f"ğŸ“Š Benchmarking Ã­ndice...")
        print(f"   ğŸ” Consultas: {len(query_vectors)}")
        print(f"   ğŸ¯ k: {k}")
        print(f"   ğŸ”„ Runs: {num_runs}")
        
        # Warm-up
        self.warmup()
        
        # Benchmark
        latencies = []
        for i in range(num_runs):
            start_time = time.time()
            scores, indices, search_time = self.search(query_vectors, k)
            latency = time.time() - start_time
            latencies.append(latency)
            
        # EstatÃ­sticas
        avg_latency = np.mean(latencies)
        min_latency = np.min(latencies)
        max_latency = np.max(latencies)
        std_latency = np.std(latencies)
        throughput = len(query_vectors) / avg_latency
        
        print(f"\nğŸ“Š Resultados do Benchmark:")
        print(f"   â±ï¸ LatÃªncia mÃ©dia: {avg_latency:.3f}s")
        print(f"   â±ï¸ LatÃªncia mÃ­nima: {min_latency:.3f}s")
        print(f"   â±ï¸ LatÃªncia mÃ¡xima: {max_latency:.3f}s")
        print(f"   ğŸ“Š Desvio padrÃ£o: {std_latency:.3f}s")
        print(f"   ğŸš€ Throughput: {throughput:.1f} consultas/segundo")
        
        return {
            "avg_latency": avg_latency,
            "min_latency": min_latency,
            "max_latency": max_latency,
            "std_latency": std_latency,
            "throughput": throughput
        }
        
    def optimize_for_rtx3060ti(self):
        """OtimizaÃ§Ãµes especÃ­ficas para RTX 3060 Ti"""
        print("ğŸ¯ Aplicando otimizaÃ§Ãµes para RTX 3060 Ti...")
        
        # ConfiguraÃ§Ãµes especÃ­ficas para RTX 3060 Ti
        rtx3060ti_config = {
            "nlist": 256,  # Otimizado para 8GB VRAM
            "nprobe": 32,  # Balance entre velocidade e precisÃ£o
            "m": 64,       # Para PQ
            "nbits": 8,    # Para PQ
            "gpu_memory_fraction": 0.9,  # 90% da VRAM
            "batch_size": 64  # Lote otimizado
        }
        
        # Atualiza configuraÃ§Ãµes
        self.config.update(rtx3060ti_config)
        
        print("âœ… OtimizaÃ§Ãµes aplicadas!")
        print(f"   ğŸ“Š nlist: {self.config['nlist']}")
        print(f"   ğŸ“Š nprobe: {self.config['nprobe']}")
        print(f"   ğŸ“Š m: {self.config['m']}")
        print(f"   ğŸ“Š nbits: {self.config['nbits']}")
        print(f"   ğŸ’¾ GPU memory: {self.config['gpu_memory_fraction']*100:.0f}%")
        
    def cleanup(self):
        """Limpa recursos"""
        if self.gpu_resources:
            del self.gpu_resources
        if self.index:
            del self.index
        torch.cuda.empty_cache()
        print("ğŸ§¹ Recursos limpos!")
